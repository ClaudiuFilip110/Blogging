{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model from scratch with explanation\n",
    "> \"A MNIST neural network classifier from scratch\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]\n",
    "- image: images/model_scratch/model.PNG\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will try to code a neural net from the ground up. We will use the basics learned from the earlier examples. Specifically, this model:\n",
    "![](my_icons/model.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "The steps we need to take are as follow:\n",
    "1. init dataset + weights and biases2\n",
    "1. calculate gradient \n",
    "    - predict the values\n",
    "    - calculate the loss\n",
    "    - calc. gradients + go backward\n",
    "    - set the gradients to 0 so they don't add up on the next backward pass\n",
    "1. calculate the accuracy\n",
    "\n",
    "## Requirements\n",
    "- a loss function\n",
    "- a model\n",
    "- scale everything up with fastai libs. and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "#first we need to get the images from the file\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "threes = (path/'train'/'3').ls() #these are paths to the files\n",
    "sevens = (path/'train'/'7').ls()\n",
    "\n",
    "#then we put them into tensors\n",
    "threes_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "sevens_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "threes_tensors[0][4:30][4:30], show_image(threes_tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, printing a value from threes_tensors we get a tensor with the value of each pixel. The form resembles a three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882, 0.9882, 0.9882, 0.8078],\n",
       "         [0.0000, 0.0000, 0.2471, 0.3686, 0.8510, 0.9922, 0.7412, 0.6196, 0.1373, 0.0784],\n",
       "         [0.0000, 0.3882, 0.9490, 0.9647, 0.8431, 0.2824, 0.0392, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.9922, 0.9882, 0.5176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.7529, 0.9922, 0.9098, 0.1843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.3059, 0.9490, 0.9882, 0.9098, 0.1647, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.4039, 0.9451, 0.9882, 0.8706, 0.2235, 0.0431, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.3843, 0.9882, 0.9922, 0.9882, 0.2784, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9922, 0.9098, 0.1843, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3843, 0.7843, 0.9882, 0.9098, 0.0000]]),\n",
       " torch.Size([6131, 28, 28]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the next step is to stack the threes and sevens together so we can get a simple array\n",
    "stacked_threes = torch.stack(threes_tensors).float()/255\n",
    "stacked_sevens = torch.stack(sevens_tensors).float()/255\n",
    "stacked_threes[0][10:20,10:20], stacked_threes.shape\n",
    "#now we can see the values are scaled between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the shape is what defins a tensor. We have 6131 images of 28 * 28 height and width. We changed it from an image to a rank 3 tensor. This will provide extremly useful later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be using <b> SGD - stochastic gradient descent </b> to teach the model to learn. \n",
    "## Steps\n",
    "\n",
    "1. *Initialize* the weights.\n",
    "1. For each image, use these weights to *predict* whether it appears to be a 3 or a 7.\n",
    "1. Based on these predictions, calculate how good the model is (its *loss*).\n",
    "1. Calculate the *gradient*, which measures for each weight, how changing that weight would change the loss\n",
    "1. *Step* (that is, change) all the weights based on that calculation.\n",
    "1. Go back to the step 2, and *repeat* the process.\n",
    "1. Iterate until you decide to *stop* the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we can finally get the final parameters that we will use\n",
    "train_x = torch.cat((stacked_threes, stacked_sevens)).view(-1,28*28)\n",
    "\n",
    "\n",
    "#we changed the rank of the tensor from 3 to 2 using view.\n",
    "#Why? Because it's easier to work with\n",
    "train_x.shape, train_x[0,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12396])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we need a label for 3 and 7. We will use 1 for 3 and 0 for 7\n",
    "#so the model will predict 1 if it's a 3 and a 0 if it's a 7\n",
    "train_y = tensor([1]*len(stacked_threes) + [0]*len(stacked_sevens))\n",
    "print(train_y.shape)\n",
    "#currently we have an array of 1s and 0s that represent all the 3s and 7s in the training set. \n",
    "#but we need a 2D array, that's required from pytorch so we use unsqueeze\n",
    "train_y = train_y.unsqueeze(1)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so the final vars. will be\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2038, 784]), torch.Size([2038, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do the same for the validation set\n",
    "\n",
    "valid_threes = (path/'valid'/'3').ls()\n",
    "valid_sevens = (path/'valid'/'7').ls()\n",
    "valid_threes_tensors = [tensor(Image.open(o)) for o in valid_threes]\n",
    "valid_sevens_tensors = [tensor(Image.open(o)) for o in valid_sevens]\n",
    "\n",
    "valid_threes_tensors = torch.stack(valid_threes_tensors).float()/255\n",
    "valid_sevens_tensors = torch.stack(valid_sevens_tensors).float()/255\n",
    "\n",
    "valid_x = torch.cat((valid_threes_tensors, valid_sevens_tensors)).view(-1, 28 * 28)\n",
    "valid_y = tensor([1] * len(valid_threes_tensors) + [0] * len(valid_sevens_tensors)).unsqueeze(1)\n",
    "\n",
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can now get the full dataset\n",
    "dset = list(zip(train_x, train_y))\n",
    "x, y = dset[0]\n",
    "\n",
    "valid_dset = list(zip(valid_x, valid_y))\n",
    "\n",
    "x.shape, y # x -> image, y -> label of that image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we need to initialize the parameters with random values\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()\n",
    "#we also need .requires_grad_ so that it knows we want back propagation + we want it in place\n",
    "\n",
    "#init. weights\n",
    "weights = init_params((28 * 28,1)) # we init. a weight for every pixel\n",
    "bias = init_params(1)\n",
    "weights.shape, bias.shape # we need this shape later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finally finished the first step in our model: initializing the random values which will change in time.\n",
    "# initialize - complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting\n",
    "The next step is to predict what the value is. How do we do that? We multiply the matrices together. By doing that we get the prediction based on the weights assigned to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0124], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_x[0] * weights.T).sum() + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the predicted value for the first image. But what we need is the prediction for the whole dataset. We can't use for loops(since they are extremly slow, but we can use broadcasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0124],\n",
       "        [12.9530],\n",
       "        [ 2.6870],\n",
       "        ...,\n",
       "        [13.2774],\n",
       "        [ 4.4847],\n",
       "        [ 9.8195]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a function that does that for us.\n",
    "def linear1(xb): return xb@weights + bias\n",
    "\n",
    "predictions = linear1(train_x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first predictions match showing that our math was correct. \n",
    "### So, we calculated the prediction using the linear function.\n",
    "The next step is to calculate the loss\n",
    "\n",
    "# Calculating the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5086318254470825"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = (predictions > 0.0).float() == train_y\n",
    "correct.float().mean().item() # this returns the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the accuracy of the model is <b> BAD </b>.\n",
    "\n",
    "AND we can't use the accuracy of the model as the loss functions since a small change in the weights will, most likely, not change the accuracy, so we need another loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4906861484050751"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x): return 1/(1+torch.exp(-x))\n",
    "\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1,1-predictions,predictions).mean()\n",
    "\n",
    "mnist_loss(predictions, train_y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the loss function is 1 - the accuracy. Now that we have finished the loss step of the road, the next thing we need to do is <b> optimization </b>.\n",
    "\n",
    "# Optimization\n",
    "In order to optimize we use batches. What are batches? \n",
    "\n",
    "<b> Batches </b> are chunks of data. We could train the model for the whole dataset, but it would take a lot of time. And we could also train the model on a single piece of data train_x[0], but that will not produce very  accurate results. So, by having this compromise we can take \"the good\" from both methods. \n",
    "\n",
    "DataLoader already has a batch_size parameter. That being said, we can start implementing everything up until now in a more concise way.\n",
    "\n",
    "\n",
    "Putting it all together\n",
    "--\n",
    "- init parameters\n",
    "- init dataLoader: dset + batch_size\n",
    "- create a func. that does prediction, loss and gradient for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0093) tensor([-0.0705])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)\n",
    "\n",
    "#dataloader\n",
    "dl = DataLoader(dset, batch_size=256)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
    "\n",
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n",
    "    \n",
    "    #why LOSS.backward()? because we need to calculate loss'(train_x). That is the gradient\n",
    "\n",
    "xb, yb = first(dl)\n",
    "calc_grad(xb, yb, linear1)\n",
    "print(weights.grad.mean(),bias.grad)\n",
    "weights.grad.zero_()\n",
    "bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5101)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6547)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#by putting this last cell together we get a function\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb,yb,model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad * lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "#now we need a function to tell us the accuracy of the model\n",
    "# xb is actually model(xb)\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = sigmoid(xb)\n",
    "    corrects = (preds>0.5) == yb\n",
    "    return corrects.float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    acc = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n",
    "    return torch.stack(acc).mean()\n",
    "\n",
    "\n",
    "#now let's try to train for an epoch and see if there is any change\n",
    "lr=1.0\n",
    "params = weights,bias\n",
    "print(validate_epoch(linear1))\n",
    "train_epoch(linear1, lr, params)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can clearly see that the improvement is massive. Clearly, we are going in the right direction. I'm wondering what would happen if we added the learning rate, instead of subtracting it. So let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6547)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8661)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_epoch_plus(model, lr, params):\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb,yb,model)\n",
    "        for p in params:\n",
    "            p.data += p.grad * lr\n",
    "            p.grad.zero_()\n",
    "            \n",
    "\n",
    "lr=1.0\n",
    "params = weights,bias\n",
    "print(validate_epoch(linear1))\n",
    "train_epoch(linear1, lr, params)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there is no difference. That's good to know. Continuing and creating a loop we can train a model for however many epochs we desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9257)\n",
      "tensor(0.9462)\n",
      "tensor(0.9540)\n",
      "tensor(0.9599)\n",
      "tensor(0.9628)\n",
      "tensor(0.9642)\n",
      "tensor(0.9662)\n",
      "tensor(0.9691)\n",
      "tensor(0.9706)\n",
      "tensor(0.9716)\n",
      "tensor(0.9716)\n",
      "tensor(0.9726)\n",
      "tensor(0.9721)\n",
      "tensor(0.9726)\n",
      "tensor(0.9736)\n",
      "tensor(0.9736)\n",
      "tensor(0.9740)\n",
      "tensor(0.9745)\n",
      "tensor(0.9755)\n",
      "tensor(0.9755)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    lr=1.0\n",
    "    params = weights,bias\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have created a model from scratch complete with a loss function, an activation function, gradient descent and everything else. Everything we created thus far has been in order to appreciate how simple models really are. PyTorch already has these functionalities, so going furthur we will use theirs instead of ours( since theirs are more optimised).\n",
    "\n",
    "# Optimizer\n",
    "We can start using PyTorch's methods. For example:\n",
    "\n",
    "nn.Linear does exactly what our init_params + linear1 does, but better. Using this information we can create our own BasicOptimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28,1)\n",
    "w, b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going further and creating the optimiser\n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None\n",
    "#again, this optimizer is just for learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(),lr)\n",
    "#thus making our train_epoch loop clearer and more concise\n",
    "def train_epoch(model):\n",
    "    for xb, yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step() # does the learning\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4932)\n",
      "tensor(0.7041)\n",
      "tensor(0.8647)\n",
      "tensor(0.9194)\n",
      "tensor(0.9375)\n",
      "tensor(0.9541)\n",
      "tensor(0.9619)\n",
      "tensor(0.9663)\n",
      "tensor(0.9672)\n",
      "tensor(0.9687)\n",
      "tensor(0.9712)\n",
      "tensor(0.9716)\n",
      "tensor(0.9731)\n",
      "tensor(0.9745)\n",
      "tensor(0.9750)\n",
      "tensor(0.9770)\n",
      "tensor(0.9770)\n",
      "tensor(0.9775)\n",
      "tensor(0.9785)\n",
      "tensor(0.9785)\n"
     ]
    }
   ],
   "source": [
    "train_model(linear_model,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusions thus far\n",
    "We have managed to create a model from the ground up. We created our own loss function, our own linear function, which we later swapped for a nn.Linear from the pyTorch library and the model from scratch; complete with batch sizes, epoch training and full model training. Finally, we created a simple optimizer for our model. \n",
    "\n",
    "This was so we can get accustomed to the way these models work. I hope I understood them correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4932)\n",
      "tensor(0.6846)\n",
      "tensor(0.8725)\n",
      "tensor(0.9199)\n",
      "tensor(0.9379)\n",
      "tensor(0.9516)\n",
      "tensor(0.9619)\n",
      "tensor(0.9658)\n",
      "tensor(0.9668)\n",
      "tensor(0.9692)\n",
      "tensor(0.9712)\n",
      "tensor(0.9716)\n",
      "tensor(0.9731)\n",
      "tensor(0.9751)\n",
      "tensor(0.9750)\n",
      "tensor(0.9765)\n",
      "tensor(0.9770)\n",
      "tensor(0.9775)\n",
      "tensor(0.9785)\n",
      "tensor(0.9789)\n"
     ]
    }
   ],
   "source": [
    "#we can swap BasicOptim for SGD which is a class in pyTorch that does the exact same thing.\n",
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(),lr)\n",
    "train_model(linear_model,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we've learned so far can be summed up in these 3 lines of code. We did this so we can understand that ML and NN are not a black box, just a tad complex, but with patience and a little determination we can understand them. Continuing in using the functionalities of fastai and pyTorch we can simplify this even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.635360</td>\n",
       "      <td>0.494356</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.226966</td>\n",
       "      <td>0.318078</td>\n",
       "      <td>0.676153</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.093637</td>\n",
       "      <td>0.142499</td>\n",
       "      <td>0.871933</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.047846</td>\n",
       "      <td>0.093609</td>\n",
       "      <td>0.921001</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.030696</td>\n",
       "      <td>0.070695</td>\n",
       "      <td>0.937684</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023929</td>\n",
       "      <td>0.057301</td>\n",
       "      <td>0.953876</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021051</td>\n",
       "      <td>0.048755</td>\n",
       "      <td>0.963199</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>0.043098</td>\n",
       "      <td>0.965653</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018820</td>\n",
       "      <td>0.039178</td>\n",
       "      <td>0.967125</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.018225</td>\n",
       "      <td>0.036301</td>\n",
       "      <td>0.969087</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017732</td>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.971050</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017297</td>\n",
       "      <td>0.032270</td>\n",
       "      <td>0.971541</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.016904</td>\n",
       "      <td>0.030767</td>\n",
       "      <td>0.973013</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016548</td>\n",
       "      <td>0.029488</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.016226</td>\n",
       "      <td>0.028387</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.027430</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.015675</td>\n",
       "      <td>0.026594</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.977429</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>0.025209</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.015035</td>\n",
       "      <td>0.024630</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataLoaders(dl, valid_dl) # this will contain all our datasets\n",
    "learner = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss,metrics=batch_accuracy)\n",
    "#now we .fit so the model can learn\n",
    "learner.fit(20, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOW\n",
    "In the end, all these libraries do is what we already know how to do. but we applied our knowledge on linear functions up until now. \n",
    "\n",
    "In order to be called Neural Networks we need a non-linear model so it can fit anything that is not linear. You'd think this would be the hardest step this far, but it is actually the easiest of them all because we have learnt so much beforehand. pyTorch helps us in that regard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = init_params((28*28,30))\n",
    "b1 = init_params(30)\n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_net(xb):\n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non linearity\n",
    "We have just created a simple neural network. It is non-linear because we have the `res = res.max(tensor(0.0))` which is a ReLU - rectified liniar unit.\n",
    "The weights shape must align\n",
    "`w1` has 30 output activations which means `w2` must have 30 input activations and 1 output, which is (of course) the result we want.\n",
    "\n",
    "ReLU sounds complicated, but all it does is replace every negative value with 0 and leaves the positive values alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(F.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `nn.Sequential` does is create a model so we can call each layer in turn. Now let's convert the linear model to a neural network model and give it a smaller learning rate( since this is a deeper model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.411041</td>\n",
       "      <td>0.504907</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.144616</td>\n",
       "      <td>0.221571</td>\n",
       "      <td>0.813543</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.079456</td>\n",
       "      <td>0.109266</td>\n",
       "      <td>0.922964</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052203</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>0.943081</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039732</td>\n",
       "      <td>0.058410</td>\n",
       "      <td>0.959274</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033423</td>\n",
       "      <td>0.049514</td>\n",
       "      <td>0.965653</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.029829</td>\n",
       "      <td>0.043923</td>\n",
       "      <td>0.966634</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.040106</td>\n",
       "      <td>0.968106</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025874</td>\n",
       "      <td>0.037340</td>\n",
       "      <td>0.969578</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024594</td>\n",
       "      <td>0.035239</td>\n",
       "      <td>0.970559</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023543</td>\n",
       "      <td>0.033573</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022650</td>\n",
       "      <td>0.032207</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021876</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.975466</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>0.030056</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.029181</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>0.028401</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019550</td>\n",
       "      <td>0.027696</td>\n",
       "      <td>0.976938</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.019103</td>\n",
       "      <td>0.027055</td>\n",
       "      <td>0.977429</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018695</td>\n",
       "      <td>0.026469</td>\n",
       "      <td>0.977429</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018321</td>\n",
       "      <td>0.025932</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017976</td>\n",
       "      <td>0.025435</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017658</td>\n",
       "      <td>0.024976</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017363</td>\n",
       "      <td>0.024549</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>0.024152</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016833</td>\n",
       "      <td>0.023783</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016595</td>\n",
       "      <td>0.023437</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.023114</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.016162</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.015965</td>\n",
       "      <td>0.022529</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>0.022264</td>\n",
       "      <td>0.981845</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>0.021780</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.015278</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.021351</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.014983</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.020789</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>0.020620</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.014468</td>\n",
       "      <td>0.020458</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.014352</td>\n",
       "      <td>0.020303</td>\n",
       "      <td>0.983808</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "#everything except the SGD is 'home made'\n",
    "learner.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final accuracy 0.9838076829910278\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pretty impressive results. We can get those results by calling learner.recorder\n",
    "plt.plot(L(learner.recorder.values).itemgot(2))\n",
    "print(f'final accuracy {learner.recorder.values[-1][2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the learning rate of the model. It's quite exponential and it is actually to be expected. Of course, on different datasets( when we go for the whole MNIST dataset) it will be different. But we'll pass that hurdle when we get there.\n",
    "\n",
    "# Final thoughts\n",
    "we've coded a neural model from scratch. I believe we've understood some fundamentally important aspects of model processing, Machine Learning and Neural Networks. The accuracy of the model is quite high for just 1-2 minutes of training AND this is a base and simple model. Using techniques that we can learn later on we'll improve these 'rookie' numbers. I look forward to that. \n",
    "\n",
    "To finish things off here is some jargon used daily in the ML world:\n",
    "- ReLU - an activation function that sets negative values to 0 and leaves positive ones untouched\n",
    "- activation function - a function that decides when a neuron should be activated /started\n",
    "- mini-batch - a batch is a small collection of the data(ex. 256 items)\n",
    "- forward pass - calculating the activation functions and loss for the layer\n",
    "- loss - a metric used by the computer to calculate how accurate our model is\n",
    "- gradient - the derivation of the loss function. This is used in the backward pass\n",
    "- backward pass - calculating the gradient and changing the values of the parameters( weights and biases).. also called a step\n",
    "- learning rate - the rate at which the system will learn.. higher values will make the model learn faster, but it is more prone to error and `bouncing`.. too small a value and you will never learn fast enough so the learning rate is a pretty important aspect of the model\n",
    "\n",
    "I've added the graph that shows how the model learns. I cannot stress this enough, this is not a black box if you understand the concepts behind it. \n",
    "\n",
    "![](my_icons/title.PNG)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
