{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks explained\n",
    "> \"An explanation of how neural networks work\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter, explanation]\n",
    "- image: images/explanation_neural_net/neural_net.PNG\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a neural network?\n",
    "A neural network is a stack of neurons who work together towards a certain goal.\n",
    "![](my_icons/neural_net.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons\n",
    "So basically, a neural net is a collection of these neurons. But what are neurons?\n",
    "A neuron is a variable that holds a value. \n",
    "What kind of value can a neuron hold?\n",
    "Most of the times a neuron just holds a value:\n",
    "\n",
    "    - it can be a value from 0 to 255 for a pixel value(for images)\n",
    "    - it can be a value of 0 or 1 for a classifier\n",
    "    - or it can be whatever value you want it to be, dependent to what you want\n",
    "    \n",
    "## basically, a neuron just holds a value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "The neurons work in layers. What is a layer? A layer is an array of neurons on the same level. In the image above ![](my_icons/layer.PNG) could be considered a layer.\n",
    "\n",
    "So a layer is just the same level of neurons. NN(Neural Networks) can have a lot of layers, different in size or length, but the basic thing to remember is that layers propagate the information further down the network. ![](my_icons/more_neural_nets.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and biases\n",
    "The next thing you need to understand are weights and biases. What are these? \n",
    "Weights and biases are part of a mathematical equation (m\\*x+n), where m IS a certain weight, n IS a certain bias and x is the value of the neuron.\n",
    "\n",
    "\n",
    "The weights and biases connect the layers together. Reffering back to the first image, you can see that every neuron in a certain layer is connected to every neuron in the next layer. These connection have certain values. \n",
    "\n",
    "Imagine a network of people. You can have a strong connection to your relatives and a weak connection to me, let's say. Well, a \"strong\" connection would have a high weight and a \"weak\" connection has a small weight. A bias also has an effect on the connection, but not quite as big as a weight since multiplication has a higher exponent than addition.\n",
    "\n",
    "```\n",
    "ex. 10 * 10 = 100 and 10 + 10 = 20\n",
    "but 10 * 100.000 = 1.000.000 and 10 + 100.000 = 100.010\n",
    "```\n",
    "So, to sum up:\n",
    "Weights and biases connect the layers together and they decide whether a neuron in the first layer has a strong connection with a neuron in the second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "Now, this is where the magic happens. What neural networks allow us to do is <b> LEARN </b> from our mistakes. We can change the parameters( weights and biases) of a neuron based on a reward function. So, if you tell the network that a connection between 2 neurons is good the network will remember that and not change the parameters a lot. But if you tell it a connection is bad, then the NN will change the parameters of that connection so the next time you go through the same neurons you will get a better result.\n",
    "\n",
    "This type of learning is called incremental learning where you take small steps by changing small things in your parameters so that only a few things will get changed. This is slow, but given enough time the network will <b> LEARN </b> to improve it's parameters and the end result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and reward\n",
    "You're probably asking yourself what determines if a connection is good or bad? What is a reward function?\n",
    "Basically, a reward function is a simple function that tells you if the output is the one that you expected.\n",
    "\n",
    "You see, neural networks rely on a type of Machine Learning called Supervised Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "Supervised learning refers to a type of machine learning where you give it a set of inputs, you tell the computer what that set of input is and you tell it \"now predict what these outputs are\". \n",
    "\n",
    "For example you can give it images of hand drawn photos(inputs) to learn from. The algorithm tries to predict what the photo is and checks the result with the label you gave it. If you say a photo is a 3 and initially it predicts 1 the parameters will change so that the next time you will get a different result. We do not know what the network changes, but we do know the output is good(this is one of the reasons it is sometimes calleda black box). You know the input, the output, but do not know how it is produced. \n",
    "\n",
    "# Going back to loss and reward\n",
    "So, the loss function is the one that determines if the connection is good or bad. And the loss functions is usually based on SGD(Stochastic Gradient Descend) - which requires more math to explain.\n",
    "\n",
    "But, in short, the loss function just says \"this output is not what i expected, change it\" or \"this is good, do not change it\".\n",
    "The next cell tries to explain SGD, so you can skip it if you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descend\n",
    "\n",
    "Gradient Descent is a iterative optimization algorithm for finding a local minimum of a differentiable function. In simpler terms, it find the minimum of a function - in our case the loss function. \n",
    "\n",
    "As you can imagine, finding the minimum of the loss functions means there is no loss. If there is no loss there is nothing left to change, thus the algorithm is complete. This would be the ideal case, but in reality usually we can't find the minimum loss.\n",
    "But how does <b> SGD </b> work?\n",
    "\n",
    "Basically, you choose a direction and a step at which you advance and try to find the minimum.\n",
    "![](my_icons/loss1.PNG) \n",
    "\n",
    "1. How do you choose the step? From my understand thus far, the step is important. Choose a too small step and you will never get to the end as the increase will be really small, choose a too big step and you risk of it \"bouncing\" and still never reaching the end.![](my_icons/loss2.PNG) I do not know yet how to choose a step.\n",
    "2. How to you choose the direction? In order to choose the direction you try to go in both directions first and see which one is closer to the minimum. for example you choose a value of x + 1 and x - 1 and see which one gets you closer to the minimum. and you choose that one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epochs and learning\n",
    "\n",
    "I said the NN learns incrementally. It does so in epochs. An epoch is the time it takes a NN to go through the whole dataset a single time. After it goes through an epoch, it goes back, with the loss function and Gradient descend and makes some small adjustments to the weights and biases, thus getting closer, step by step to the minimum loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    " \n",
    "There are a lot more things to explain such as activation functions, back propagation, feed forward etc, but this is my understanding this far of NN.\n",
    "\n",
    "Neural Networks learn by changing the weights and biases between different neurons. These values are changed based on the loss function which is calculated using a method called Stochastic Gradient Descend. \n",
    "\n",
    "\n",
    "In conclusion, neural networks are extremly complicated and my \"explanation\" doesn't even come close to the reality, but it could be considered a starting point towards understand these neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
